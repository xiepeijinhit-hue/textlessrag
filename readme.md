



#  <div align="center"> <img width="50" height="50" alt="learning" src="https://github.com/user-attachments/assets/bc41e430-0565-43ee-b479-095fe83c9a87" /> TextLessRAG: End-to-end Visual Document RAG by Speech without Text<div>

<div align="center">
<!-- <h1>A Multi-round Multi-modal Reinforcement Learning Framework</h1> -->
<p><strong>An end2end OCR & ASR & TTS free multimodal retrieval-augmented generation pipeline </strong></p>

<a href='https://huggingface.co/datasets/hit12345/textlessrag'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Datasets-green'></a>
<a href="https://huggingface.co/vidore/colqwen-omni-v0.1" target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Retriver-blue></a>
<a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B" target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Generator-blue></a>
<a href="https://huggingface.co/collections/juliozhao/doclayout-yolo-670cdec674913d9a6f77b542" target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Layout-yellow></a>
<a href="https://arxiv.org/abs/2509.07538" target="_blank"><img src=https://img.shields.io/badge/Paper-arXiv-red></a>


</div>

## Introduction
We introduce **TextlessRAG**, the first end-to-end framework for speech-based question answering over large-scale document images. TextlessRAG bypasses ASR, TTS, and OCR, directly interpreting spoken queries, retrieving relevant visual knowledge, and generating answers in a fully textless pipeline. 

We release the first bilingual speechâ€“document RAG dataset **SV-DOC**, containing Chinese and English voice queries aligned with multimodal document content, to foster future research in this direction.

## ğŸ–¥ï¸ Example
We demonstrate an example of a pipeline in the video below.

https://github.com/user-attachments/assets/27884988-2bc6-4aa2-b892-6eaa39baa005

### Example List 
we provide examples of Chinese doc rag in [examples](dataset/CHN_examples.md) and English part in [data](dataset/readme.md)


## ğŸ”¥ News
- ğŸ‰  We propose TextlessRAG, the first speech-based, end-to-end visual document RAG pipeline that operates without ASR, OCR, or TTS.
- ğŸ‰  We construct SV-DOC, the first speech-based visual document RAG benchmark, featuring bilingual data in English and Chinese.
- â³ The project is still under ongoing development, and the code includingï¼šknowedge embedding, topk retrieval, layout reranking, QA generation will be available soon~
<!-- - âŒ›ï¸ Training code will be released soon. -->
<!-- - ğŸ‰ Our framework integrates various embedding models, enabling you to create your own retriever.
- ğŸ‰ We have released the ViDoSeek dataset, which is suitable for Retrieval-augmented Generation in the large visually rich document collection. -->

### List of Contents
- [x] Provide the environment setup with model weights.
- [x] Provide the details of DataEngine with Examples.
- [ ] Provide the Crops of audio and document images.
- [ ] Provide the meta data.  
        



## ğŸš€ Quick Start

### Create the Python environment.
```python
conda create -n textlessrag python=3.11.10
conda activate textlessrag
pip install git+https://github.com/illuin-tech/colpali.git #Colqwen-Omni
pip install git+https://github.com/QwenLM/Qwen2.5-Omni.git  #Qwen2.5-Omni
pip install -r requirements.txt
```
### Download CKP from Hugging Face 
Download `Retriver`ï¼Œ`Generator` and `LayOut` model weights from Hugging Face to folder `models`. 
| model | github | ckp |
|------|------|------|
| Colqwen-Omni | [GitHub](https://github.com/illuin-tech/colpali) | [![ğŸ¤— HF](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-orange)](https://huggingface.co/vidore/colqwen-omni-v0.1) |
| Qwen2.5-Omni | [GitHub](https://github.com/QwenLM/Qwen2.5-Omni) | [![ğŸ¤— HF](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-orange)](https://huggingface.co/Qwen/Qwen2.5-Omni-7B) |
| DocLayout-YOLO | [GitHub](https://github.com/opendatalab/DocLayout-YOLO?tab=readme-ov-file) | [![ğŸ¤— HF](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-orange)](https://huggingface.co/collections/juliozhao/doclayout-yolo-670cdec674913d9a6f77b542) |

## ğŸ¤– TextLessRAG Pipeline
<img width="500" height="350" alt="pipeline" src="https://github.com/user-attachments/assets/d6fc78bc-9838-4758-a3b0-8f1d1d93060c" />


### <img width="20" height="20" alt="search" src="https://github.com/user-attachments/assets/72fdf7fd-278d-42e9-98d4-18e65e048210" /> Offline knowledge embedding and layout  ã€todoã€‘
Layout Dictionary
```python
id_2_class_name = {
0: 'title',
1: 'plain text',
2: 'abandon',
3: 'figure',
4: 'figure caption',
5: 'table',
6: 'table caption',
7: 'table footnote',
8: 'isolate formula',
9: 'formula caption'
}
```

### <img width="30" height="30" alt="book" src="https://github.com/user-attachments/assets/96e8fdff-ea18-40fe-8f25-6c7875f1a94d" /> Online query embedding, topk retrieval and reranking  ã€todoã€‘

## ğŸ“Š Data Engine   
Details of data engine with refine rules and examples are provided in [refine rules](data.md) and [data examples](dataset/readme.md)
### Data Statistics
Among them, the chartqa, dude, infovqa, and slidevqa datasets are not the original data but filtered subsets, directly sourced from the open-source dataset in the CVPR 2025 paper VDocRAG: [https://vdocrag.github.io](https://vdocrag.github.io)

| Dataset          | Amount | Pool (pdf amount / image amount) | Content                          | Domain     |
|------------------|--------|----------------------------|----------------------------------|------------|
| chartqa          | 150    | 119                        | Chart                            | Academic   |
| dude             | 496    | 422                        | Chart / Table / Text             | Open       |
| infovqa          | 1048   | 300                        | Chart / Table / Layout / Image   | Infographic|
| slidevqa         | 760    | 657                        | Chart / Table / Image / Text     | Slide      |
| mmlongbench-doc  | 1091   | 135 / 5134                 | Chart / Table / Layout / Image / Text | Open  |
| vidoseek         | 1142   | 290 / 5349                 | Chart / Table / Layout / Image   | Open       |
| ä¸­æ–‡bench         | 1260   | 737 / 30583                | Chart / Table / Layout / Image / Text | Open  |
| **Total**        | **5947** | **42564**                 | Chart / Table / Layout / Image / Text | Open |

### Five-step data generation
<img width="500"  alt="data" src="https://github.com/user-attachments/assets/2ec70aa0-01ba-4197-b7aa-ab176a6203ef" />

####  Step 1: Data Dollection 
We collect a diverse set of documents and reports across multiple domains and formats, including
PDFs and PPTS. 
####  Step 2: Layout
We utilize Doc-Layout YOLO to split whole page doc images into fine-grain blocks.
####  Step 3: QA Generate
The Layout blocks are processed by commercial visionâ€“language agents(QwenVL and GPT-4o) via batch APIs to generate candidate QA pairs.
####  Step 4: Refine
The QA pairs are refined through rule-based filtering and professional human annotation.
####  Step 5: TTS
The text QA pairs are converted into speech using Doubaoâ€™s TTS API, leveraging over 200 professional voice types.
  

   
### <img width="20" height="20" alt="document" src="https://github.com/user-attachments/assets/f2932f31-5402-45f1-83c4-b3e05a842eaf" /> English Visual Doc RAG Datasets


### <img width="20" height="20" alt="google-docs" src="https://github.com/user-attachments/assets/69446942-0193-4953-85f9-d0e08d95f6f3" /> Chinese Visual Doc RAG Dataset
We construct the first Chinese Visual Doc RAG Dataset -- ChineseDocRAG(CDR)
#### <img width="15" height="15" alt="bar-chart" src="https://github.com/user-attachments/assets/25cc5ea5-8fac-4b14-a83c-f596d8246863" /> Page Distribution
<img width="4000" height="600" alt="histogram" src="https://github.com/user-attachments/assets/0bb4d5b2-c22f-44c2-add1-02c021b5c1b7" />


- Domains world cloud:

<img width="500" height="300" alt="wordcloud" src="https://github.com/user-attachments/assets/556408ce-8d54-41e2-9bca-a8494a8879ee" />

- PDF Count Across Domains

| Domain | Count |
| ---- | ---- |
| åŸæŠ•&å»ºæŠ•&åŸå»º | 76 |
| äº¤é€šå‡ºè¡Œ&è¿è¾“ | 94 |
| æ–°èƒ½æºè½¦ä¼ | 110 |
| åˆ›æ–°&åˆ›ä¸š&åˆ›å®¢ | 52 |
| æœºæ¢°è®¾å¤‡ | 111 |
| å®è§‚ç»æµ&ä¸­å›½ç»æµ | 115 |
| æœæ±&é¥®æ–™ | 131 |
| å…¬å‹ŸREITs | 116 |
| å¤§å¥åº·&å¥åº·äº§ä¸š&å¥åº·é£Ÿå“ | 89 |
| æ°”å€™&ç¯å¢ƒ&ç¯ä¿ | 81 |
| åª’ä½“ | 137 |
| ä¸­æ¦‚äº’è”ç½‘å…¬å¸ | 23 |
| å‡ºæµ· | 80 |
| çŸ¿å±±&çŸ¿äº§&çŸ¿ä¸š | 124 |
| è‚¡ç¥¨ | 111 |
| è´¢æ”¿ | 122 |
| æœè£…&æœé¥°&å¥³è£…&ç«¥è£… | 111 |
| ä½ç©ºç»æµ | 97 |
| å® ç‰©&èŒå® ç»æµ | 124 |
| å°çº¢ä¹¦è¿è¥&ç©æ³• | 109 |
| ç”Ÿæ€ | 93 |
| äººåŠ›èµ„æº&äººæ‰æ‹›è˜&è–ªé…¬&ç»©æ•ˆ | 107 |
| ä½ç¢³ | 113 |
| æ³•å¾‹ | 110 |
| å¤§æ•°æ®&å¤§æ•°æ®å®‰å…¨ | 125 |
| æ•°å­—åŒ–&æ•°å­—åŒ–è½¬å‹ | 91 |
| é”€å”® | 101 |
| å®¶å±…&å®¶å…·&å®¶è£…&å…¨å±‹å®šåˆ¶&æ™ºèƒ½å®¶å±… | 127 |
| ç”µå­å•†åŠ¡&ç›´æ’­ç”µå•†&è·¨å¢ƒç”µå•† | 76 |
| ç§‘æŠ€ | 58 |
| ææ–™ | 52 |
| åŠå¯¼ä½“&èŠ¯ç‰‡ | 69 |
| æ¶ˆè´¹&æ–°æ¶ˆè´¹ | 76 |
| è½¯ä»¶ | 141 |
| æŠ–éŸ³&TikTok | 142 |
| æ—…æ¸¸&æ–‡æ—…&ä¼‘é—²æ™¯åŒº | 140 |
| ä¸œå—äºš | 127 |
| æ¬§æ´²&æ¬§ç›Ÿ | 90 |
| ç”ŸçŒªå…»æ®–&å…»çŒª&çŒªå‘¨æœŸ | 96 |
| èˆ¹èˆ¶&èˆªè¿&æ¸¯å£ | 123 |
| ç‰©æµ&å¿«é€’&æ™ºæ…§ç‰©æµ | 93 |
| è®¡ç®—æœº | 92 |
| å¯è½¬å€º | 109 |
| æ”¿ä¼&å›½ä¼&å¤®ä¼ | 110 |
| ç”µæ± &é”‚ç”µæ± &é’ ç”µæ± &åŠ¨åŠ›ç”µæ±  | 102 |
| å…»æ®–ä¸š | 72 |
| ç¨€åœŸ | 83 |
| çŸ³æ²¹åŒ–å·¥&çŸ³åŒ– | 143 |
| ç½‘ç»œå®‰å…¨&ç‰©è”ç½‘å®‰å…¨&äº’è”ç½‘å®‰å…¨ | 118 |
| å›½äº§åŒ–&å›½äº§æ›¿ä»£ | 86 |
| å…¬å‹ŸåŸºé‡‘ | 117 |
| é’¢é“&é’¢æ | 120 |
| å…ƒå®‡å®™&è™šæ‹Ÿç°å® | 115 |
| æ°¢èƒ½æº | 142 |
| å…¨çƒç§‘æŠ€å…¬å¸ | 80 |
| æ–°ææ–™ | 99 |
| ä¸œæ•°è¥¿ç®—&ç®—åŠ› | 58 |
| çŒå¤´&HR | 124 |
| ä½“è‚²äº§ä¸š&ä½“è‚²è¡Œä¸š&ä½“è‚²æ¶ˆè´¹ | 143 |
| åˆ©ç‡å¤–æ±‡ | 100 |
| é‡‘å±é“œ | 106 |
| æˆ¿åœ°äº§&æˆ¿è´·&REITs | 101 |
| åŸºå»ºè¡Œä¸š&åŸºç¡€è®¾æ–½å»ºè®¾ | 107 |
| æ”¯ä»˜è¡Œä¸š&ç¬¬ä¸‰æ–¹æ”¯ä»˜&è·¨å¢ƒæ”¯ä»˜ | 103 |
| å€ºåˆ¸&åŸæŠ•å€º&ä¿¡ç”¨å€º | 90 |
| ç¾å¦†&ä¸ªæŠ¤&åŒ–å¦†å“ | 123 |
| ä¿é™© | 101 |
| æ™ºèƒ½å®¶ç”µ&ç”µå™¨&å°å®¶ç”µ | 126 |
| é‡‘è&ç»¿è‰²é‡‘è&ç¢³é‡‘è | 67 |
| äººå·¥æ™ºèƒ½&ChatGPT&AIGC&ç”Ÿæˆå¼AI | 59 |
| çœ¼é•œ&çœ¼ç§‘ | 115 |
| é€šä¿¡ | 73 |
| æœ‰è‰²é‡‘å± | 66 |
| ç¢³äº¤æ˜“&ç¢³èµ„äº§&ç¢³ç›˜æŸ¥&ç¢³å¸‚åœº&ç¢³è¾¾å³°&ç¢³ä¸­å’Œ | 112 |
| ç å®&é¦–é¥°&é¥°å“ | 19 |
| é›¶å”®&æ–°é›¶å”® | 82 |
| è™šæ‹Ÿç”µå‚&ç”µåŠ›ç³»ç»Ÿ | 110 |
| å¹¶è´­ | 119 |
| æ‰‹æœºç”¨æˆ·&ç§»åŠ¨æ¶ˆè´¹è€… | 37 |
| å›½é˜²å†›å·¥&èˆªå¤©è£…å¤‡&å¤§é£æœº | 48 |
| æ™ºèƒ½åˆ¶é€  | 72 |
| å·¥ä¸š4.0 | 95 |
| æ–°è´¨ç”Ÿäº§åŠ› | 123 |
| æ–°èƒ½æºæ±½è½¦&å……ç”µæ¡© | 112 |
| èµ„æœ¬å¸‚åœº | 111 |
| ç†è´¢&é“¶è¡Œç†è´¢&äº’è”ç½‘ç†è´¢ | 125 |
| ESG | 109 |
| å…¨çƒèµ„é‡‘è§‚å¯Ÿç³»åˆ—æŠ¥å‘Š | 116 |
| æ™ºæ…§å…»è€&å…»è€é‡‘&å…»è€é‡‘è | 94 |
| åˆ¶è¯&åˆ¶è¯è£…å¤‡ | 50 |
| æœºå™¨äºº&å®¶åº­æœºå™¨äºº&å•†ç”¨æœºå™¨äºº&äººå½¢æœºå™¨äºº&åŒ»ç–—æœºå™¨äºº | 68 |
| ç”µå•†è´­ç‰©èŠ‚&åŒ11&618 | 123 |
| ç¾å›½&ç¾å›½ç»æµæ°‘ç”Ÿ&ç¾å›½æ¶ˆè´¹è€… | 54 |
| çººç»‡å“&çººç»‡æœè£…&å®¶çºº&æ£‰çººç»‡æŠ¥å‘Š | 132 |
| æ—¥æœ¬å¯ç¤ºå½•&æ—¥æœ¬ç»æµ&ä»¥æ—¥ä¸ºé‰´ | 87 |
| æ•™è‚²&èŒä¸šæ•™è‚²&æ™ºæ…§æ•™è‚²&åœ¨çº¿æ•™è‚²&æ•™åŸ¹ | 128 |
| çŸ­è§†é¢‘&ç›´æ’­ | 122 |
| è¶Šå—&å°åº¦&æ—¥æœ¬ | 81 |
| èˆªç©ºè¿è¾“&èˆªç©ºç‰©æµ | 83 |
| VC&PE | 134 |
| åŒ»ç–—&åŒ»è¯ | 31 |
| ç”µåŠ›&é£ç”µ&ç«ç”µ&æ ¸ç”µ | 72 |
| æ•°æ®ä¸­å¿ƒIDC | 30 |
| æ¶ˆè´¹è€…&äººç¾¤&ç”»åƒ&ç”¨æˆ· | 32 |
| ç¤¾ä¼šæœåŠ¡ | 141 |
| ä¹˜ç”¨è½¦ | 133 |
| è´µé‡‘å±&é»„é‡‘&ç™½é“¶ | 26 |
| å‚¨èƒ½ | 110 |
| äº‘åŸç”Ÿ&å¼€æº&å¼€æºè½¯ä»¶ | 72 |
| å°±ä¸š | 130 |
| é«˜è´¨é‡å‘å±• | 97 |
| åˆ¸å•†è¯åˆ¸ | 50 |
| é‡‘å±é“ | 94 |
| æ±½è½¦ | 98 |
| å…‰ä¼&å¤ªé˜³


## Citation

If you use this repository in your work, please cite:

```bibtex
@misc{xie2025textlessragendtoendvisualdocument,
      title={TextlessRAG: End-to-End Visual Document RAG by Speech Without Text}, 
      author={Peijin Xie and Shun Qian and Bingquan Liu and Dexin Wang and Lin Sun and Xiangzheng Zhang},
      year={2025},
      eprint={2509.07538},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.07538}, 
}


